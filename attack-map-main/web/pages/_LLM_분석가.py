# web/pages/2_LLM_ë¶„ì„ê°€.py
import streamlit as st
import json
import requests
import pandas as pd
from pathlib import Path

# ê³µìš© ëª¨ë“ˆ ë° í•¨ìˆ˜ import
from ui_components import setup_page, display_sidebar, display_footer
from data_handler import load_events
from config import LLM_BASE_URL, LLM_MODEL, LLM_TIMEOUT, OLLAMA_MODEL

# í˜ì´ì§€ ì „ìš© UI ë° ë¡œì§ 

def get_llm_response_stream(context_df: pd.DataFrame, user_query: str):
    """LLM ì„œë²„ì— ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ì„ ë³´ë‚´ê³  ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ yieldí•˜ëŠ” í•¨ìˆ˜."""
    if context_df.empty:
        yield "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        return

    ctx_records = context_df.to_dict(orient="records")
    ctx_json = json.dumps(ctx_records, ensure_ascii=False, default=str)
    
    # ì„œë²„ ì¢…ë¥˜ íŒë³„
    try:
        requests.get(f"{LLM_BASE_URL}/v1/models", timeout=2)
        mode = "openai"
    except Exception:
        mode = "ollama"

    try:
        if mode == "openai":
            payload = {
                "model": LLM_MODEL,
                "messages": [
                    {"role": "system", "content": "ë„ˆëŠ” ë³´ì•ˆ ë¡œê·¸ ë¶„ì„ ì „ë¬¸ê°€ë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ, ì£¼ì–´ì§„ ë¡œê·¸(context)ì— ê¸°ë°˜í•´ì„œë§Œ ìš”ì•½/ì„¤ëª…í•´ë¼. ì¶”ì¸¡ì´ë‚˜ ì™¸ë¶€ ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆë¼."},
                    {"role": "user", "content": f"ë‹¤ìŒ ìµœê·¼ 5ê°œ ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ ë¶„ì„í•´ì„œ ë‹µí•´ì¤˜.\n\ncontext:\n{ctx_json}\n\nì§ˆë¬¸:\n{user_query}"}
                ], 
                "temperature": 0.2, 
                "max_tokens": 1024,
                "stream": True  # ìŠ¤íŠ¸ë¦¬ë° ì˜µì…˜ í™œì„±í™”
            }
            resp = requests.post(f"{LLM_BASE_URL}/v1/chat/completions", json=payload, timeout=LLM_TIMEOUT, stream=True)
            resp.raise_for_status()
            
            for line in resp.iter_lines():
                if line:
                    decoded_line = line.decode('utf-8')
                    if decoded_line.startswith('data: '):
                        json_str = decoded_line[6:]
                        if json_str.strip() == '[DONE]':
                            break
                        try:
                            chunk = json.loads(json_str)
                            content = chunk.get("choices", [{}])[0].get("delta", {}).get("content")
                            if content:
                                yield content
                        except json.JSONDecodeError:
                            continue
        else: # Ollama
            payload = {
                "model": OLLAMA_MODEL, 
                "messages": [{"role": "user", "content": f"context:\n{ctx_json}\n\nì§ˆë¬¸:\n{user_query}"}], 
                "stream": True # ìŠ¤íŠ¸ë¦¬ë° ì˜µì…˜ í™œì„±í™”
            }
            resp = requests.post(f"{LLM_BASE_URL}/api/chat", json=payload, timeout=LLM_TIMEOUT, stream=True)
            resp.raise_for_status()

            for line in resp.iter_lines():
                if line:
                    chunk = json.loads(line.decode('utf-8'))
                    content = chunk.get("message", {}).get("content")
                    if content:
                        yield content

    except requests.exceptions.ReadTimeout:
        yield f"â±ï¸ íƒ€ì„ì•„ì›ƒ: ì„œë²„ê°€ {LLM_TIMEOUT}ì´ˆ ë‚´ì— ì‘ë‹µí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ì´ ë¡œë“œ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
    except Exception as e:
        yield f"âŒ LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}\n ì„œë²„ ì£¼ì†Œ({LLM_BASE_URL})ì™€ ëª¨ë¸ ì‹¤í–‰ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."


# ë©”ì¸ ì‹¤í–‰ ë¡œì§
def main():
    setup_page("ğŸ’¬", "LLM ê¸°ë°˜ ë¡œê·¸ ë¶„ì„ê°€")
    events_path = display_sidebar()
    df = load_events(str(events_path))
    
    st.markdown("ìµœì‹  ê³µê²© ì´ë²¤íŠ¸ 5ê°œë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ìƒí™©ì„ ìš”ì•½**í•˜ê±°ë‚˜ **ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ìµœì‹  ë³´ì•ˆ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ëŠ” AIì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]

    col1, col2 = st.columns([2, 1])

    with col2:
        st.markdown("#### ğŸ“œ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸")
        st.info("AIëŠ” ì•„ë˜ì˜ ìµœì‹  ì´ë²¤íŠ¸ 5ê°œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.", icon="â„¹ï¸")
        context_df = df.sort_values("ts", ascending=False).head(5)
        st.dataframe(context_df[['ts', 'src_ip', 'country_code', 'label']], hide_index=True, use_container_width=True)
        
        with st.expander("ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ ë³´ê¸°"):
            st.markdown("""
            - "ê°€ì¥ ë§ì´ ë°œìƒí•œ ê³µê²© ìœ í˜•ì€ ë­ì•¼?"
            - "Seychellesì—ì„œ ë“¤ì–´ì˜¨ ê³µê²©ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜."
            - "IP `120.26.230.64`ëŠ” ëª‡ ë²ˆì´ë‚˜ ì ‘ì†í–ˆì–´?"
            - "ì „ì²´ì ì¸ ìƒí™©ì„ ìš”ì•½í•´ì¤˜."
            """)

    with col1:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # AI ì‘ë‹µì„ ìƒì„±í•˜ê³  í‘œì‹œ
            with st.chat_message("assistant"):
                # st.write_streamì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ í‘œì‹œ
                response_generator = get_llm_response_stream(context_df, prompt)
                full_response = st.write_stream(response_generator)
            
            # ì „ì²´ ì‘ë‹µì„ ë°›ì€ í›„, ì„¸ì…˜ ê¸°ë¡ì— ì¶”ê°€
            st.session_state.messages.append({"role": "assistant", "content": full_response})

    display_footer()

if __name__ == "__main__":
    main()